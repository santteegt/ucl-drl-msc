\chapter{Background}
\label{chapterlabel2}

\section{Reinforcement Learning}

Reinforcement learning involves modifying the behavior of a system such that the expected value of some possibly delayed scalar feedback is maximized.

Reinforcement learning [7, 19] is a family of machine learning algorithms that optimize sequential decision making
processes based on scalar evaluations called reinforcements. The objective is to maximize the expected sum of future reinforcements for each state.

If for each state the best action is taken according to the estimated value function, then the policy is called greedy.

An MDP is a model for sequential stochastic decision problems where an autonomous agent is influencing its surrounding environment through actions

Q-Learning in which values are estimated for each state and action combination

Exploration/Exploitation
exploitation corresponds with the generation of trust and the exploration with the presentation of new and unexpected items

the exploitation makes the behavior of the users more predictable, making estimates of the values more reliable. The exploration makes sure that the users have a chance to visit all pages, since it allows for all alternatives to be tried

\section{Collaborative Filtering}

use background patterns of \textbf{An MDP-Based Recommender System} 

\subsection{Matrix Factorization}

In a recommendation system such as Netflix or MovieLens, there is a group of users and a set of items (movies for the above two systems). Given that each users have rated some items in the system, we would like to predict how the users would rate the items that they have not yet rated, such that we can make recommendations to the users

the task of predicting the missing ratings can be considered as filling in the blanks such that the values would be consistent with the existing ratings in the matrix.

The intuition behind using matrix factorization to solve this problem is that there should be some latent features that determine how a user rates an item. Hence, if we can discover these latent features, we should be able to predict a rating with respect to a certain user and a certain item, because the features associated with the user should match with the features associated with the item.

In trying to discover the different features, we also make the assumption that the number of features would be smaller than the number of users and the number of items

we have a set U of users, and a set D of items. Let $\mathbf{R}$ of size $|U| \times |D|$ be the matrix that contains all the ratings that the users have assigned to the items. Also, we assume that we would like to discover K latent features. Our task, then, is to find two matrics matrices $\mathbf{P}$ (a $|U| \times K$ matrix) and $\mathbf{Q}$ (a $|D| \times K$ matrix) such that their product approximates $\mathbf{R}$:

\begin{center}
$\mathbf{R} \approx \mathbf{P} \times \mathbf{Q^{T}} = \hat{\mathbf{R}}$
\end{center}

Each row of $\mathbf{P}$ would represent the strength of the associations between a user and the features. Similarly, each row of $\mathbf{Q}$ would represent the strength of the associations between an item and the features. We have to find a way to obtain $\mathbf{P}$ and $\mathbf{Q}$. One way to approach this problem is the first intialise the two matrices with some values, and calculate how 'different? their product is to $\mathbf{M}$ by using gradient descent with a regularization term in order to find the minimum difference between $\mathbf{R}$ and the predicted preference matrix $\mathbf{\hat{R}}$ (using the MSE).The update rules and the cost function are defined as follows:

\begin{center}
$p^{'}_{ik}=p_{ik}+\alpha\frac{\partial}{\partial p_{ik}}e^{2}_{ij}=p_{ik}+\alpha(2e_{ij}q_{kj}-\beta p_{ik})$
$q^{'}_{kj}=q_{kj}+\alpha\frac{\partial}{\partial q_{kj}}e^{2}_{ij}=q_{kj}+\alpha(2e_{ij}p_{ik}-\beta p_{kj})$

$e^2_{ij}=(r_{ij}-\hat{r{ij}})^2=(r_{ij}-\sum_{k=1}^{K}p^{'}_{ik}q^{'}_{kj})^2$
\end{center}


\section{Collaborative Filtering and Reinforcement Learning}

\textbf{Exploration/Exploitaition in Adaptive Recommender Systems} considered reinforcement learning with an exploration/exploitation approach of unknown areas to automatically improve their recommendation policy for helping users to navigate through an adaptive web site and showed that a greedy policy can indeed lead to suboptimal recommending. They state that a model free algorithm like Q-Learning can become infeasible if the number of actions is so high. Additionally, the use of a greedy policy, where the best actions are taken, does not always improve the policy, so to avoid reinforcing values for states with initial high values, a random exploration process needs to be added to the action selection. The popular exploration $\epsilon$ greedy policy was used where the greedy action is selected with probability ($1-\epsilon$) and an arbitrary other action with probability $\epsilon$ otherwise. However, the exploration/exploitation dilemma comes out as the exploration makes the future of a sequence less predictable influencing the estimation of the value function itself but inconsistent random actions have to be chosen to obtain a better policy. This problem is solved by starting with a lot of exploration and gradually reducing $\epsilon$ when the policy is getting closer to the optimum. Finally, their experiments concluded that a recommender system without exploration potentially can get trapped into a local maxima.

\textbf{New Recommendation System Using Reinforcement Learning} presented a general framework for web recommendation that learns directly from customer's past behavior by applying an RL process which uses the SARSA method[***ref here] and a $\epsilon$-greedy policy. The system was composed of two models: a global model for all customers to keep track of the trends as a whole, and a local model to record the user's individual browsing history. In general, the model treats pages as states of the system and links within a page as actions. To predict the next state, it will use a ranking system that is also separated into 2 parts. First, a global ranking system which uses the data from a $Q_{global}$-matrix (obtained from the action-value function) to choose the next state by using an $\epsilon$-greedy policy. It gives a chance for new items that have few clicks but may match a user's interest. The second part is composed by a local ranking $Q_{local}$ which uses an inverse $\epsilon$-greedy policy to estimate the next state. It gives the customer more chance to explore other products than the ones he already visited. Finally, the system finds the final total reward using $Q_{total}=Q_{local} + wQ_{global}$ where $w \in (0-1]$ is a weight hyper parameter. 

Authors performed some experiments to find a relationship between $\epsilon$ and the user click rate. The findings showed that a value of $\epsilon=0.2$ preserves the balance between exploration and exploitation, meanwhile If $\epsilon < 0.2$ the system will gave small chances to users to explore new items, or may include products that does not match to the customer's interest otherwise ($\epsilon > 0.2$). Although the purpose of $Q_{local}$ was to promote new products, it was less effective than $Q_{global}$. In conclusion, the result of this experiment explained that if a user is allowed to do much exploration, he/she has a chance to find non-relevant products, but if users just exploit the system, they have a chance to just keep receiving the same recommended items over time.

\textbf{An MDP-Based Recommender System} 

We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two benefits: they take into account the long-termeffects of each recommendation and the expected value of each recommendation

an MDP-based recommender system must employ a strong initialmodel,must be solvable quickly, and should not consume toomuchmemory

it is the first to report experimental analysis conducted on a real commercial site

we suggest that recommendation is not simply a sequential prediction problem, but rather, a sequential decision problem

With this view in mind, a more sophisticated approach to recommender systems emerges. First,
one can take into account the utility of a particular recommendation. Second, we might suggest an item whose immediate reward is lower, but leads to more likely or more profitable rewards in the future
These considerations are taken into account automatically by any good or optimal policy gen-
erated for anMDP model of the recommendation process

The benefits of an MDP-based recommender system discussed above are offset by the fact
that the model parameters are unknown
they take considerable time to converge and their initial behavior is random.

The approach we suggest initializes a predictive model of user behavior using data gathered on the site prior to the implementation of the recommender system. We then use the predictive model to provide initial parameters for theMDP

The predictive model can be thought of as a (first-order)Markov chain in which states correspond to sequences of events

In this paper, we consider model- based systems for collaborative filtering

at each stage a new list is calculated based on the user?s past ratings, will lead us naturally to our reformulation of the recommendation process as a sequential optimization process. optimal rec-
ommendations may depend not only on previous items pruchased, but also on the order in which those items are purchased

policy-iteration (Howard, 1960), which we use in our imple- mentation

Unfortunately, these recent methods do not seem applicable in our domain in which the structure of the state space is quite different ? that is, each state can be viewed as an assignment to a very small number of variables (three in the typical case) each with very large domains. Moreover, the values of the variables (describing items bought recently) are correlated

However, we were able to exploit the special structure of our state and action spaces using different techniques. In addition, we introduce approximations that exploit the fact that most states ? that is, most item sequences ? are highly unlikely to occur

then an
MDP can be used to model user behavior with recommendations

**The Model**
Our first step is to construct a predictive model of user purchases
does not take into account its influence on the user
use a Markov chain, with an appropriate formulation of the state space
The implemented Markov chain is a model of user ?dynamics.?. we need to formulate an appropriate notion of a user state and to estimate the state-transition function
**States represent relevant information about users in the form of ordered sequence of previous selections. Ignore data such as age, gender but it could be beneficial (through creating handcrafted features for such large domain variables)
Problems arise: ?data sparsity and MDP solution complexity
we consider only sequences of at most k items, for some relatively small value of k, an represented as vector of length k
The initial state in theMarkov chain is the state in which every entry has the value missing
use values of k from 1 to 5
**transition funcion describes the probability that a user whose k recent selections were x1, . . . ,xk will select the item x? next
Initially, this transition function is unknown to us
a maximum-likelihood estimate can be used

trMC(?x1,x2,x3?,?x2,x3,x4?)= count(?x1,x2,x3,x4?) / count(?x1,x2,x3)
This model, however, still suffers from the problem of data sparsity and performs poorly in practice








**An MDP based recommender model
MDP model that explicitly models the recommendation process and attempts to optimize it. The predictive model plays an important role in the construction of this model
The predictive model provides the probability the user will purchase a particular item x given that her sequence of past purchases is x1, . . . ,xk
We denote this value by Prpred(x|x1, . . . ,xk), where k = 3 in our case

states: k-tuples of items
actions: correspond to a recommendation of a single item.
Rewards: depends on the last item defining the current state only. we use net profit for reward

The state following each recommendation is determined by the user?s response to that recommendation. the user has three options:
Accept this recommendation, thus transferring from state ?x1,x2,x3? into ?x2,x3,x'? 
Select some non-recommended item x'', thus transferring the state ?x1,x2,x3? into ?x2,x3,x''?.
Select nothing (for example, when the user terminates the session), in which case the system remains in the same state.

transition function: the stochastic element in our model is the user?s actual choice
tr1 MDP(?x1,x2,x3?,x',?x2,x3,x''?) is the probability that the user will select item x?? given that item x? is recommended in state ?x1,x2,x3?

A for-profit e-commerce8 site is unlikely to use a recommender system that generates irrelevant recommendations for a long period, while waiting for it to converge to an optimal policy. We therefore need to initialize the transition function carefully. We can do so based on any good predictive model

need to initialize the transition function carefully. using the following assumptions
-A recommendation increases the probability that a user will buy an item. This probability is proportional to the probability that the user will buy this item in the absence of recom- mendations
We denote the proportionality constant for recommendation r in state s by ?s,r, where ?s,r >1
-The probability that a user will buy an item that was not recommended is lower than the probability that she will buy when the system issues no recommendations at all
We denote the proportionality constant for recommendation r in state s by ?s,r, where ?s,r <1


We use trpredict(s,s · r) to denote the probability that the userwill choose r next, given that its current state is s according to the predictive model in which recommendations are not considered, that is, Prpred(r|s)
--copy transition probability functions page 16

we can represent this transition function concisely using at most two values for every state-item pair
Because the number of items is much smaller than the number of states, we obtain significant reduction in the space requirements of the model.

Our state space enjoys a number of features that lead to fast convergence of the policy iteration algorithm:
-Transitions in our state space seem to have inherent directionality
-We have also found that the computation of an optimal policy is not heavily sensitive to variations in k?the number of past transactions we encapsulate in a state. As k increases, so does the number of states, but the number of positive entries in our transition matrix remains similar.

policy evaluation still requires much effort given the large state and action space we have to deal with. To alleviate this problem we resort to a number of approximations:
-ignoring unobserved states by just we maintain transition probabilities only for states for which a transition occurred in our training data. if the sequence ?x,y,z? did not appear in the training data, we do not calculate a policy for it and assume its value to be R(z)?the reward for the last item in the sequence
When a recommendation must be generated for a state that was not encountered in the past,
we compute the value of the policy for this state online. This requires us to estimate the transition probabilities for a state that did not appear in our training data using the techniques of skipping, clustering, and finite mixture of unigram, bigram, and trigrams
-using the independence of recommendations At each iteration, we compute the best action for each state s ? that is, the action satisfying:
maybe copy function page 19 formula 24

**Updating the model online
One approach is to use some form of reinforcement learning but the implementation requires many calls and computations by the recommender systemonline, which will lead to slower responses?an undesirable result. A simpler approach is to perform off-line updates at fixed time intervals

In particular, the system does not recommend
items that are likely to be bought whether recommended or not, but rather recommends items whose likelihood of being purchased is increased when they are recommended
Approach used: The site need only keep track of the recommendations and the user selections and, say, once a week use those statistics to build a new model

formulas 29-36

prior to updating the model, the system is not able to recommend those new items (the well-known ?cold start? prob- lem(Good et al., 1999) in recommender systems)

In our implementation,when the first transition to a state s · r is observed, its probability is initialized to 0.9 the probability of the most likely next item in state s with ?s =10. This approach causes the new items to be recommended quite frequently

the system needs to recommend non-optimal items occasionally in order to get counts for those items. The systembalances the need to explore unobserved options in order to improve its model and the desire to exploit the data it has gathered so far in order to get rewards
In our implementation we use a list of three recommendations where the first one is always the optimal one, but the second and third items are selected using the Boltzman distribution without a cutoff


Weaknesses of our predictive (Markov chain) model include the use of ad hoc weighting func-
tions for skipping and similarity functions and the use of fixed mixture weights
Learning the weighting functions and mixture weights from data should improve calibration


\textbf{A Hybrid Web Recommender System Based on Q-Learning} we exploit this idea to enhance a reinforcement learning framework,
primarily devised for web recommendations based on web usage data
show the apt and flexibility of the reinforcement learning framework in the web recommendation domain and demonstrate how it can be extended in order to incorporate various sources of information.

Web mining discover user preferences from their implicit feedbacks, namely the web pages they have visited

In this paper we exploit this idea to enhance a reinforcement learning solution, we had devised for web recommendations based on web usage data [16].
but this method suffers from the problems commonly faced by other usage-based techniques

To address these problems, we made use of the conceptual relationships among web pages and derived a novel model of the problem, enriched with semantic knowledge about the usage behavior
Then we came up with new definitions for our states, actions and rewarding functions which
capture the semantic
implications of users browsing behavior
shows the flexibility of the reinforcement learning framework for the recommendation problem and how it can be extended to incorporate other sources of information.

\textbf{Improving adaptation of ubiquitous recommander systems by using reinforcement learning and collaborative filtering} Inspired by models of human reasoning developed in robotic, we combine reinforcement learning and case-based reasoning to define a \textbf{contextual} recommendation process based on different context dimensions (cognitive, social, temporal, geographic). This paper describes an ongoing work on the implementation of a CBRS based on a hybrid Q-learning (HyQL) algorithm which combines Q-learning, collaborative filtering and case-based reasoning techniques.

When applying techniques to adapt a recommender system to the user, major difficulties for ensuring the user acceptance follow
-Avoiding the intervention of experts: experts are not completely sure of the user?s interest
A slow learning process: the system?s learning process about the user?s interests has to be fast enough to avoid bothering the user with incorrect recommendations.
-The evolution of the user?s interest: The system has to be continuously adapted to this dynamicity using the user?s context information to provide the relevant recommendations because, if the system?s behavior is incoherent, the user refuses it quickly.

The system?s learning process has to be fast to adequately follow the evolution of the user?s interest

the recommender system starts with a predefined set of actions, not defined by an expert, but by the user?s social group (in the scenario we talk about job teams).
Then, the system is progressively adapted to a particular user using a learning lifelong process. Thus, the system is, at first, only acceptable to the user, and will, as time passes, give more and more satisfying results

Avoiding the intervention of experts: in [8, 24] the authors use Reinforcement Learning (RL) because it does not need previous experiences to start work. However, a major difficulty when applying RL techniques to real world problems is their slow convergence.

Paper proposes to address the following problems that came out in recommender systems
-Avoiding the intervention of experts: we propose to use the Q- learning algorithm which does not need initial user?s information.
-Reducing cold start problem: we give Q-learning algorithm the ability to explore the knowledge of other users belonging to the same social network by using CF.
-Accelerating the learning process: to accelerate the Q-learning process, we mix Q-learning with case-based reasoning techniques to allow the reuse of cases and faster satisfy the user.
-Adapting to the user?s interest evolution: we propose to use CF with an exploration strategy to follow the user?s interest evolution.

The proposition is based on three main algorithms

-Reinforcement learning and Q learning algorithm
In the Q-Learning algorithm, for every state s, action a = Q (s) is chosen according to the current policy. The choice of the action by the policy must ensure a balance between exploration and exploitation phase

There are several strategies to make the balance between exploration and exploitation. Here, we focus on two of them: the greedy strategy chooses always the best action from the Q-table, i.e. uses only exploitation; the ?-greedy strategy adds some greedy exploration policy, choosing a random action at each step if the policy returns the greedy action (probability = ?) or a random action (probability = 1 - ?).

-Collaborative Filtering
Given a set of transactions D, where each transaction T identified by id is of the form <id, item, rating>, a recommender model M is produced. Such a model M can produce a list of top-N recommended items, and corresponding predicted ratings, from a given set of known ratings
In terms of CF, three major classes of algorithms exist: Memory- based, Model-based and Hybrid-based [1, 4]. Memory-based approaches identify the similarity between two users by comparing their ratings on a set of items and have suffered from two fundamental problems: sparsity and scalability. Alternatively, the model-based approaches have been proposed to improve these problems, but these approaches tend to limit the range of users. In our work, we use the Hybrid-based approach which combines
the advantages of these two kinds of approaches by joining the two methods. Firstly, we employ memory-based CF to fill the vacant ratings of the user-item matrix. Then, we use the item- based CF as model-based to form the nearest neighbors of each item

-Case-based reasoning
Case based reasoning (CBR) uses knowledge of previous situations (cases) to solve new problems, by finding a similar past case and reusing it in the new problem situation.
According to [11], solving a problem with CBR involves ?obtaining a problem description, measuring the similarity of the current problem to previous problems stored in a case base with their known solutions, retrieving one or more similar cases, and attempting to reuse the solution of the retrieved case(s), possibly after adapting it to consider differences in problem descriptions?.

-The Hybrid Q-learning algorithm
Q-learning Improved by 
Reusing past cases information: to accelerate the Q-learning algorithm, we propose to integrate CBR into each iteration. Before choosing the best action, the algorithm computes the similarity between the present case and each one in the case base and, if there is a case that can be reused, the algorithm retrieves and adapts it.
Using social groups: to give the Q-Learning the ability to use information from other users sharing the same interests, we propose to extend the ?-greedy strategy replacing the random action by another one that is selected computing the similarity of user profiles applying the CF algorithm as indicated in equation 2 where q is a random value uniformly distributed over [0, 1] and p (0?p?1) is a parameter that defines the exploration/exploitation tradeoff: the larger is p, the smaller is the probability of executing a random exploratory action. a social group is an action chosen among those

**Global mechanism. Figure 1
implementation of a context-based recommender system (CBRS) which uses the HyQL algorithm
The sensing module detects time, location, cognitive and social dimensions of the context in the following way
-The cognitive dimension is given by all the actions of the user
-The social group is predefined for each user
-Time is detected by the user?s phone and the calendar of his/her institution.
-The geographic dimension is detected by the user?s GPS

In the thinking module, the abstraction phase is based on inference rules (e.g. specification / generalization) defined on the temporal and/or space ontologies
The aggregation phase is the combination of the two dimensions time and location
The reasoning module chooses an action to deliver at each situation. 
The reasoning module is controlled by each of the previously presented algorithms: Q-Learning and HyQL

Evaluation
comparing Q-learning and HyQL w.r.t. solving the cold start problem
The experiment consists of testing the first 100 trials of the system starting when the user is connected to the system. During the trials, the system has to recommend one resource from the database
To evaluate each trial, we use the traditional precision measure
In general, the precision of HyQL is greater than the precision of Q- Learning, except for trials number 50 where they have the same value

\textbf{ENHANCING COLLABORATIVE FILTERING MUSIC RECOMMENDATION BY BALANCING EXPLORATION AND EXPLOITATION} this work is based on a content-based approach, introduce exploration into CF and try to balance between exploration and exploitation. The approach uses a Bayesian graphical model that takes account of both CF latent factors and recommendation novelty. Moreover, they designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions.

Carried out both simulation experiments and a user study to show the efficiency and effectiveness of the proposed approach. Experimental results showed that our proposed approach
enhances the performance of CF-based music recommen- dation significantly

With the appropriate amount of exploration, the recommender system could gain more knowledge about the user?s true preferences before exploiting them.

They introduce exploration into collaborative filtering by formulating the music recommendation problem as a reinforcement learning task called \textit{n-armed bandit problem}. 

Compared to an off-the-shelf MCMC algorithm, a much more efficient sampling algorithm is proposed to speed up Bayesian posterior estimation

A Bayesian graphical model taking account of both collaborative filtering latent factors and recommendation novelty is proposed to learn the user pref- erences
\section{Deep Reinforcement Learning}

% This just dumps some pseudolatin in so you can see some text in place.
% \blindtext
